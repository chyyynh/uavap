{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **一、安裝所需套件**"
   ],
   "metadata": {
    "id": "ke3OVBraXoqp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "!pip install -U ultralytics rasterio geopandas shapely torch torchvision torchaudio opencv-python tqdm matplotlib"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12FczB5bXvuR",
    "outputId": "a72612af-4b46-4755-f938-70f603e7dcbe"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.241-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.4.4)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.11.12)\n",
      "Requirement already satisfied: click!=8.2.*,>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio) (1.1.1.2)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.12.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading ultralytics-8.3.241-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.1/899.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:01:30\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m150.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m144.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
      "    unknown package:\n",
      "        Expected sha256 27331cd902fb4322252657f3902adf1c4f6acad9dcad81d8df3ae14c7c4f07c4\n",
      "             Got        475e877ada31ce2805f7b1346eb3e42f172fcf07903ada8cfcec624793660b3b\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **二、下載模型權重檔**"
   ],
   "metadata": {
    "id": "SISkOY66lhoL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown --fuzzy https://drive.google.com/file/d/1N-Ge9PbHqGskKV7EzMrGZznEGhVYi8Af/view?usp=sharing -O Model-20251222T144950Z-3-001.zip\n",
    "\n",
    "!unzip -q Model-20251222T144950Z-3-001.zip -d /content/Model\n",
    "!rm Model-20251222T144950Z-3-001.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXE0WLSHkBWP",
    "outputId": "33298c10-cdbb-4e06-ea12-da75019f191b"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1N-Ge9PbHqGskKV7EzMrGZznEGhVYi8Af\n",
      "From (redirected): https://drive.google.com/uc?id=1N-Ge9PbHqGskKV7EzMrGZznEGhVYi8Af&confirm=t&uuid=4fa980b6-9bd5-4ee6-9827-4b1076dffd86\n",
      "To: /content/Model-20251222T144950Z-3-001.zip\n",
      "100% 137M/137M [00:02<00:00, 56.5MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "MODELS = {\n",
    "    \"car\": {\n",
    "        \"path\": \"/content/Model/Model/vehicle.pt\",\n",
    "        \"conf\": 0.75,\n",
    "        \"patch_size\": 1024,\n",
    "        \"overlap\": 950,\n",
    "        \"nms_iou\": 0.0,\n",
    "        \"color\": (255,0,0)\n",
    "    },\n",
    "    \"person\": {\n",
    "        \"path\": \"/content/Model/Model/human.pt\",\n",
    "        \"conf\": 0.60,\n",
    "        \"patch_size\": 1024,\n",
    "        \"overlap\": 950,\n",
    "        \"nms_iou\": 0.0,\n",
    "        \"color\": (0,0,255)\n",
    "    },\n",
    "    \"cone\": {\n",
    "        \"path\": \"/content/Model/Model/cone.pt\",\n",
    "        \"conf\": 0.50,\n",
    "        \"patch_size\": 640,\n",
    "        \"overlap\": 480,\n",
    "        \"nms_iou\": 0.35,\n",
    "        \"color\": (0,255,255)\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "id": "3y2NAgydeYYd"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **三、上傳影像**"
   ],
   "metadata": {
    "id": "kbrseklydSZd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload the image file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename from the uploaded dictionary\n",
    "if uploaded:\n",
    "    filename = next(iter(uploaded))\n",
    "    ORTHO_TIF = f\"/content/{filename}\"\n",
    "    print(f\"Uploaded file: {filename}\")\n",
    "    print(f\"ORTHO_TIF is set to: {ORTHO_TIF}\")\n",
    "else:\n",
    "    print(\"No file was uploaded.\")\n",
    "    ORTHO_TIF = \"\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "fKJRI8nudSIg",
    "outputId": "2a07d24f-14ae-4f3c-bb65-95cd77cb7398"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-d96c7ede-9290-405b-b64f-c233c81eae3d\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-d96c7ede-9290-405b-b64f-c233c81eae3d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving odm_orthophoto.tiff to odm_orthophoto.tiff\n",
      "Uploaded file: odm_orthophoto.tiff\n",
      "ORTHO_TIF is set to: /content/odm_orthophoto.tiff\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **四、執行人/車/角錐模組**"
   ],
   "metadata": {
    "id": "_rmSBWw_XeVx"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "82c385e0",
    "outputId": "c2c6fe8b-79f0-43e1-c18b-8ce1f52b3e19"
   },
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- PATH & CONFIG ----------------\n",
    "#ORTHO_TIF = \"/content/gdrive/MyDrive/1141220_Object_Detection/Data/ortho/odm_orthophoto_600.tif\"\n",
    "OUT_ROOT  = Path(\"GIS_Output\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- OBIA RULES ----------------\n",
    "OBIA_RULES = {\n",
    "    \"car\":   {\"area\": (6, 35),   \"aspect\": (1.0, 2.2)},\n",
    "    \"person\":{\"area\": (0.2, 1.3),\"aspect\": (0.5, 2.0)},\n",
    "    \"cone\":  {\"area\": (0.05,0.5),\"aspect\": (0.8, 1.4)}\n",
    "}\n",
    "\n",
    "# ================= LOAD IMAGE & MODELS =================\n",
    "src = rasterio.open(ORTHO_TIF)\n",
    "transform = src.transform\n",
    "width, height = src.width, src.height\n",
    "pixel_width_m, pixel_height_m = src.res\n",
    "\n",
    "models_yolo = {k: YOLO(v[\"path\"]) for k,v in MODELS.items()}\n",
    "\n",
    "# ================= SLIDING WINDOW INFERENCE =================\n",
    "raw_detections = []\n",
    "\n",
    "for cls_name, cfg in MODELS.items():\n",
    "    model = models_yolo[cls_name]\n",
    "    step = cfg[\"patch_size\"] - cfg[\"overlap\"]\n",
    "\n",
    "    for y in tqdm(range(0, height, step), desc=f\"Inference {cls_name} Rows\"):\n",
    "        for x in range(0, width, step):\n",
    "            win_w = min(cfg[\"patch_size\"], width - x)\n",
    "            win_h = min(cfg[\"patch_size\"], height - y)\n",
    "\n",
    "            patch = src.read(window=Window(x, y, win_w, win_h))\n",
    "            patch = np.transpose(patch, (1,2,0))[:,:,:3]\n",
    "\n",
    "            results = model.predict(patch, conf=cfg[\"conf\"], verbose=False)\n",
    "\n",
    "            for r in results:\n",
    "                if r.boxes is None:\n",
    "                    continue\n",
    "                boxes  = r.boxes.xyxy.cpu().numpy()\n",
    "                scores = r.boxes.conf.cpu().numpy()\n",
    "\n",
    "                for i,(x1,y1,x2,y2) in enumerate(boxes):\n",
    "                    raw_detections.append({\n",
    "                        \"class\": cls_name,\n",
    "                        \"conf\": float(scores[i]),\n",
    "                        \"px1\": x1 + x,\n",
    "                        \"py1\": y1 + y,\n",
    "                        \"px2\": x2 + x,\n",
    "                        \"py2\": y2 + y\n",
    "                    })\n",
    "\n",
    "print(f\"\\nRaw detections: {len(raw_detections)}\")\n",
    "\n",
    "# ================= GLOBAL NMS =================\n",
    "final_detections = []\n",
    "\n",
    "for cls in MODELS:\n",
    "    cls_raw = [r for r in raw_detections if r[\"class\"] == cls]\n",
    "    if not cls_raw:\n",
    "        continue\n",
    "\n",
    "    boxes  = torch.tensor([[r[\"px1\"],r[\"py1\"],r[\"px2\"],r[\"py2\"]] for r in cls_raw])\n",
    "    scores = torch.tensor([r[\"conf\"] for r in cls_raw])\n",
    "\n",
    "    keep = nms(boxes, scores, MODELS[cls][\"nms_iou\"])\n",
    "    for i in keep:\n",
    "        final_detections.append(cls_raw[int(i)])\n",
    "\n",
    "print(f\"After NMS: {len(final_detections)}\")\n",
    "\n",
    "# ================= OBIA + GEO =================\n",
    "records = []\n",
    "id_counter = {k:0 for k in MODELS}\n",
    "\n",
    "for r in final_detections:\n",
    "    w_m = (r[\"px2\"]-r[\"px1\"]) * pixel_width_m\n",
    "    h_m = (r[\"py2\"]-r[\"py1\"]) * pixel_height_m\n",
    "    area = w_m * h_m\n",
    "    aspect = max(w_m,h_m)/(min(w_m,h_m)+1e-6)\n",
    "\n",
    "    a_min,a_max = OBIA_RULES[r[\"class\"]][\"area\"]\n",
    "    r_min,r_max = OBIA_RULES[r[\"class\"]][\"aspect\"]\n",
    "    if not (a_min <= area <= a_max and r_min <= aspect <= r_max):\n",
    "        continue\n",
    "\n",
    "    id_counter[r[\"class\"]] += 1\n",
    "    cx = (r[\"px1\"]+r[\"px2\"])/2\n",
    "    cy = (r[\"py1\"]+r[\"py2\"])/2\n",
    "    gx,gy = transform*(cx,cy)\n",
    "\n",
    "    corners = [\n",
    "        transform*(r[\"px1\"],r[\"py1\"]),\n",
    "        transform*(r[\"px2\"],r[\"py1\"]),\n",
    "        transform*(r[\"px1\"],r[\"py2\"]),\n",
    "        transform*(r[\"px2\"],r[\"py2\"])\n",
    "    ]\n",
    "    xs,ys = zip(*corners)\n",
    "\n",
    "    records.append({\n",
    "        \"class\": r[\"class\"],\n",
    "        \"id\": id_counter[r[\"class\"]],\n",
    "        \"confidence\": round(r[\"conf\"],3),\n",
    "        \"area_m2\": round(area,2),\n",
    "        \"aspect_ratio\": round(aspect,2),\n",
    "        \"center_x\": round(gx,3),\n",
    "        \"center_y\": round(gy,3),\n",
    "        \"geometry\": box(min(xs),min(ys),max(xs),max(ys)),\n",
    "        \"bbox_px\": (int(r[\"px1\"]),int(r[\"py1\"]),int(r[\"px2\"]),int(r[\"py2\"]))\n",
    "    })\n",
    "\n",
    "gdf = gpd.GeoDataFrame(pd.DataFrame(records), geometry=\"geometry\", crs=src.crs)\n",
    "print(f\"After OBIA Filter: {len(gdf)} detections\")\n",
    "\n",
    "# ╧ 顯示表格（你要的那個）\n",
    "print(\"\\nDetected Objects GeoDataFrame (first 5 rows):\")\n",
    "display(gdf.head(25))\n",
    "\n",
    "# ================= VISUALIZATION =================\n",
    "vis = src.read([1,2,3])\n",
    "vis = np.transpose(vis,(1,2,0))\n",
    "vis = cv2.cvtColor(vis.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "for _,r in gdf.iterrows():\n",
    "    x1,y1,x2,y2 = r[\"bbox_px\"]\n",
    "    color = MODELS[r[\"class\"]][\"color\"]\n",
    "    cv2.rectangle(vis,(x1,y1),(x2,y2),color,2)\n",
    "    cv2.circle(vis,((x1+x2)//2,(y1+y2)//2),3,color,-1)\n",
    "    cv2.putText(\n",
    "        vis,\n",
    "        f'{r[\"class\"]}-{r[\"id\"]} {r[\"confidence\"]:.2f}',\n",
    "        (x1, max(y1-5,15)),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.5,\n",
    "        color,\n",
    "        1\n",
    "    )\n",
    "\n",
    "out_png = \"all_objects_combined.png\"\n",
    "cv2.imwrite(str(out_png),vis)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(cv2.cvtColor(vis,cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Combined YOLOv8 Object Detection\")\n",
    "plt.show()\n",
    "\n",
    "# ================= EXPORT GIS =================\n",
    "gdf_export = gdf.drop(columns=[\"bbox_px\"])\n",
    "\n",
    "for cls in MODELS:\n",
    "    out = OUT_ROOT/cls\n",
    "    out.mkdir(exist_ok=True)\n",
    "    sub = gdf_export[gdf_export[\"class\"]==cls]\n",
    "\n",
    "    sub[['class','id','confidence','area_m2','center_x','center_y']] \\\n",
    "        .to_csv(out/f\"{cls}.csv\", index=False)\n",
    "    sub.to_file(out/f\"{cls}.geojson\", driver=\"GeoJSON\")\n",
    "    sub.to_file(out/f\"{cls}.shp\")\n",
    "\n",
    "# ================= DETECTION SUMMARY (COUNTS ONLY) =================\n",
    "print(\"\\n--- Detection Summary ---\")\n",
    "total = 0\n",
    "for cls in MODELS:\n",
    "    print(f\"{cls}: {id_counter[cls]} detections\")\n",
    "    total += id_counter[cls]\n",
    "print(f\"\\nTotal Objects Detected (all classes): {total}\")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1380504629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ],
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbc6c239"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous code execution successfully addressed the `OUT_ROOT` definition issue. The request now is to explicitly define the subtask and its instructions in a text cell, which matches the existing content of cell `23b0e891`. I will confirm this by modifying the cell with the provided text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "88942839",
    "outputId": "288ec415-172e-481f-c205-1fca55866441"
   },
   "source": [
    "## Remove OUT_ROOT and define local output directory\n",
    "\n",
    "### Subtask:\n",
    "Modify cell `foIxOTfbXm4J` to remove all references to `OUT_ROOT`. Instead, define a local output directory (e.g., 'GIS_Output') within the cell and use it for saving the exported GIS files and their respective class subdirectories. This ensures that output files are saved without needing a global `OUT_ROOT` variable.\n",
    "\n",
    "#### Instructions\n",
    "1. Navigate to cell `foIxOTfbXm4J`.\n",
    "2. Locate the commented-out lines related to `OUT_ROOT` under the 'PATH & CONFIG' section.\n",
    "3. Uncomment the line that defines `OUT_ROOT` and modify its value to `OUT_ROOT = Path(\"GIS_Output\")`.\n",
    "4. Uncomment the line `OUT_ROOT.mkdir(parents=True, exist_ok=True)` to ensure the directory is created."
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ipython-input-2036268953.py, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2036268953.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Modify cell `foIxOTfbXm4J` to remove all references to `OUT_ROOT`. Instead, define a local output directory (e.g., 'GIS_Output') within the cell and use it for saving the exported GIS files and their respective class subdirectories. This ensures that output files are saved without needing a global `OUT_ROOT` variable.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install fastapi uvicorn nest_asyncio -q\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYEdGJJxoGUK",
    "outputId": "a0c88733-13cc-46c4-a725-1bec6926a7e5"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selecting previously unselected package cloudflared.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
      "Unpacking cloudflared (2025.11.1) ...\n",
      "Setting up cloudflared (2025.11.1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "SY9A6e-PoF6q"
   }
  },
  {
   "cell_type": "code",
   "source": "import threading\nimport subprocess\nimport time\nimport re\nimport os\nimport shutil\nimport uvicorn\nimport socket\nimport rasterio\nfrom rasterio.windows import Window\nimport numpy as np\nimport cv2\nimport torch\nfrom torchvision.ops import nms\nfrom ultralytics import YOLO\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import box\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom fastapi import FastAPI, File, UploadFile, BackgroundTasks\nfrom fastapi.responses import Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom pyproj import Transformer\nfrom io import BytesIO\nfrom PIL import Image, ImageDraw\n\n# ----- 全域變數 -----\ngdf = None\nsrc = None\nORTHO_TIF = \"\"\northo_png_cache = None  # 快取 PNG 資料\nprocessing_status = {\n    \"status\": \"idle\",  # idle, running, done, error\n    \"progress\": 0,\n    \"current_step\": \"\",\n    \"message\": \"\"\n}\n\n# ----- 檢查 port 是否可用 -----\ndef is_port_in_use(port):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex(('localhost', port)) == 0\n\nif is_port_in_use(8000):\n    print(\"⚠️ Port 8000 已被占用，請先停止上一個 API cell 再重新執行\")\n    raise SystemExit(\"Port 8000 already in use\")\n\n# ----- 載入模型 -----\nprint(\"正在載入 YOLO 模型...\")\nmodels_yolo = {k: YOLO(v[\"path\"]) for k, v in MODELS.items()}\nprint(\"✅ 模型載入完成\")\n\n# ----- OBIA 規則 -----\nOBIA_RULES = {\n    \"car\":   {\"area\": (6, 35),   \"aspect\": (1.0, 2.2)},\n    \"person\":{\"area\": (0.2, 1.3),\"aspect\": (0.5, 2.0)},\n    \"cone\":  {\"area\": (0.05,0.5),\"aspect\": (0.8, 1.4)}\n}\n\n# ----- 生成正射影像 PNG -----\ndef generate_ortho_png():\n    \"\"\"將 GeoTIFF 轉換為 PNG（縮小尺寸以加快傳輸）\"\"\"\n    global ortho_png_cache\n    \n    if src is None:\n        return None\n    \n    try:\n        # 讀取影像（最多 4096x4096）\n        max_size = 4096\n        scale = min(max_size / src.width, max_size / src.height, 1.0)\n        \n        out_width = int(src.width * scale)\n        out_height = int(src.height * scale)\n        \n        # 使用 rasterio 的 out_shape 來縮放\n        img = src.read(\n            out_shape=(src.count, out_height, out_width),\n            resampling=rasterio.enums.Resampling.bilinear\n        )\n        \n        # 轉換為 RGB\n        if img.shape[0] >= 3:\n            rgb = np.transpose(img[:3], (1, 2, 0)).astype(np.uint8)\n        else:\n            rgb = np.transpose(np.stack([img[0]]*3), (1, 2, 0)).astype(np.uint8)\n        \n        # 編碼為 PNG\n        _, buffer = cv2.imencode('.png', cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))\n        ortho_png_cache = buffer.tobytes()\n        \n        print(f\"✅ 已生成正射影像 PNG ({out_width}x{out_height})\")\n        return ortho_png_cache\n        \n    except Exception as e:\n        print(f\"❌ 生成 PNG 失敗: {e}\")\n        return None\n\n# ----- 偵測函數 -----\ndef run_detection():\n    \"\"\"執行完整的偵測流程\"\"\"\n    global gdf, src, processing_status\n    \n    try:\n        processing_status = {\"status\": \"running\", \"progress\": 0, \"current_step\": \"Loading image\", \"message\": \"\"}\n        \n        # 載入影像\n        src = rasterio.open(ORTHO_TIF)\n        transform = src.transform\n        width, height = src.width, src.height\n        pixel_width_m, pixel_height_m = src.res\n        \n        # 生成 PNG 快取\n        generate_ortho_png()\n        \n        processing_status[\"progress\"] = 10\n        processing_status[\"current_step\"] = \"Running inference\"\n        \n        # 執行推論\n        raw_detections = []\n        total_classes = len(MODELS)\n        \n        for idx, (cls_name, cfg) in enumerate(MODELS.items()):\n            model = models_yolo[cls_name]\n            step = cfg[\"patch_size\"] - cfg[\"overlap\"]\n            \n            processing_status[\"current_step\"] = f\"Detecting {cls_name}\"\n            \n            for y in range(0, height, step):\n                for x in range(0, width, step):\n                    win_w = min(cfg[\"patch_size\"], width - x)\n                    win_h = min(cfg[\"patch_size\"], height - y)\n                    \n                    patch = src.read(window=Window(x, y, win_w, win_h))\n                    patch = np.transpose(patch, (1,2,0))[:,:,:3]\n                    \n                    results = model.predict(patch, conf=cfg[\"conf\"], verbose=False)\n                    \n                    for r in results:\n                        if r.boxes is None:\n                            continue\n                        boxes = r.boxes.xyxy.cpu().numpy()\n                        scores = r.boxes.conf.cpu().numpy()\n                        \n                        for i, (x1,y1,x2,y2) in enumerate(boxes):\n                            raw_detections.append({\n                                \"class\": cls_name,\n                                \"conf\": float(scores[i]),\n                                \"px1\": x1 + x, \"py1\": y1 + y,\n                                \"px2\": x2 + x, \"py2\": y2 + y\n                            })\n            \n            processing_status[\"progress\"] = 10 + int((idx + 1) / total_classes * 50)\n        \n        processing_status[\"progress\"] = 60\n        processing_status[\"current_step\"] = \"Applying NMS\"\n        \n        # Global NMS\n        final_detections = []\n        for cls in MODELS:\n            cls_raw = [r for r in raw_detections if r[\"class\"] == cls]\n            if not cls_raw:\n                continue\n            boxes = torch.tensor([[r[\"px1\"],r[\"py1\"],r[\"px2\"],r[\"py2\"]] for r in cls_raw])\n            scores = torch.tensor([r[\"conf\"] for r in cls_raw])\n            keep = nms(boxes, scores, MODELS[cls][\"nms_iou\"])\n            for i in keep:\n                final_detections.append(cls_raw[int(i)])\n        \n        processing_status[\"progress\"] = 80\n        processing_status[\"current_step\"] = \"OBIA filtering\"\n        \n        # OBIA + GEO\n        records = []\n        id_counter = {k: 0 for k in MODELS}\n        \n        for r in final_detections:\n            w_m = (r[\"px2\"]-r[\"px1\"]) * pixel_width_m\n            h_m = (r[\"py2\"]-r[\"py1\"]) * pixel_height_m\n            area = w_m * h_m\n            aspect = max(w_m,h_m)/(min(w_m,h_m)+1e-6)\n            \n            a_min, a_max = OBIA_RULES[r[\"class\"]][\"area\"]\n            r_min, r_max = OBIA_RULES[r[\"class\"]][\"aspect\"]\n            if not (a_min <= area <= a_max and r_min <= aspect <= r_max):\n                continue\n            \n            id_counter[r[\"class\"]] += 1\n            cx = (r[\"px1\"]+r[\"px2\"])/2\n            cy = (r[\"py1\"]+r[\"py2\"])/2\n            gx, gy = transform * (cx, cy)\n            \n            corners = [\n                transform*(r[\"px1\"],r[\"py1\"]),\n                transform*(r[\"px2\"],r[\"py1\"]),\n                transform*(r[\"px1\"],r[\"py2\"]),\n                transform*(r[\"px2\"],r[\"py2\"])\n            ]\n            xs, ys = zip(*corners)\n            \n            records.append({\n                \"class\": r[\"class\"],\n                \"id\": id_counter[r[\"class\"]],\n                \"confidence\": round(r[\"conf\"], 3),\n                \"area_m2\": round(area, 2),\n                \"aspect_ratio\": round(aspect, 2),\n                \"center_x\": round(gx, 3),\n                \"center_y\": round(gy, 3),\n                \"geometry\": box(min(xs), min(ys), max(xs), max(ys)),\n                \"bbox_px\": (int(r[\"px1\"]), int(r[\"py1\"]), int(r[\"px2\"]), int(r[\"py2\"]))\n            })\n        \n        gdf = gpd.GeoDataFrame(pd.DataFrame(records), geometry=\"geometry\", crs=src.crs)\n        \n        processing_status = {\n            \"status\": \"done\",\n            \"progress\": 100,\n            \"current_step\": \"Complete\",\n            \"message\": f\"偵測完成！共 {len(gdf)} 個物件\"\n        }\n        print(f\"✅ 偵測完成！共 {len(gdf)} 個物件\")\n        \n    except Exception as e:\n        processing_status = {\n            \"status\": \"error\",\n            \"progress\": 0,\n            \"current_step\": \"Error\",\n            \"message\": str(e)\n        }\n        print(f\"❌ 偵測失敗: {e}\")\n\n# ----- 建立 API -----\napp = FastAPI(title=\"UAV AIP API\")\napp.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n\n# ----- 類別名稱對應 -----\nCLASS_MAP = {\"car\": \"vehicle\", \"person\": \"person\", \"cone\": \"cone\"}\n\n# ----- 從 gdf 轉換真實偵測資料 -----\ndef get_real_detections():\n    if gdf is None or len(gdf) == 0 or src is None:\n        return []\n    \n    try:\n        transformer = Transformer.from_crs(src.crs, \"EPSG:4326\", always_xy=True)\n    except:\n        return []\n    \n    detections = []\n    for _, row in gdf.iterrows():\n        lon, lat = transformer.transform(row[\"center_x\"], row[\"center_y\"])\n        detections.append({\n            \"id\": int(row[\"id\"]),\n            \"cls\": CLASS_MAP.get(row[\"class\"], row[\"class\"]),\n            \"score\": float(row[\"confidence\"]),\n            \"center_x\": float(row[\"center_x\"]),\n            \"center_y\": float(row[\"center_y\"]),\n            \"area_m2\": float(row[\"area_m2\"]),\n            \"aspect_rat\": float(row[\"aspect_ratio\"]),\n            \"elev_z\": 0.0,\n            \"height_m\": 0.0,\n            \"lat\": float(lat),\n            \"lon\": float(lon),\n        })\n    return detections\n\n# ----- API 端點 -----\n@app.get(\"/\")\nasync def root():\n    return {\"status\": \"ok\", \"message\": \"UAV AIP API 運行中\"}\n\n@app.post(\"/api/upload\")\nasync def upload_image(file: UploadFile = File(...)):\n    global ORTHO_TIF, src, ortho_png_cache\n    upload_path = f\"/content/{file.filename}\"\n    with open(upload_path, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n    ORTHO_TIF = upload_path\n    \n    # 載入影像並生成 PNG\n    src = rasterio.open(ORTHO_TIF)\n    generate_ortho_png()\n    \n    print(f\"✅ 已上傳: {file.filename}\")\n    return {\"filename\": file.filename, \"message\": \"上傳成功\"}\n\n@app.post(\"/api/process\")\nasync def start_process(background_tasks: BackgroundTasks):\n    \"\"\"啟動偵測處理\"\"\"\n    global processing_status\n    \n    if not ORTHO_TIF:\n        return {\"error\": \"請先上傳影像\"}\n    \n    if processing_status[\"status\"] == \"running\":\n        return {\"error\": \"偵測正在進行中\"}\n    \n    # 在背景執行偵測\n    background_tasks.add_task(run_detection)\n    processing_status = {\"status\": \"running\", \"progress\": 0, \"current_step\": \"Starting\", \"message\": \"\"}\n    \n    return {\"status\": \"started\", \"message\": \"偵測已啟動\"}\n\n@app.get(\"/api/process/status\")\nasync def get_process_status():\n    \"\"\"取得處理狀態\"\"\"\n    return processing_status\n\n@app.get(\"/api/detections/{project_id}\")\nasync def get_detections(project_id: str):\n    return get_real_detections()\n\n@app.get(\"/api/projects\")\nasync def get_projects():\n    if ORTHO_TIF:\n        return [{\"id\": \"current\", \"name\": ORTHO_TIF.split(\"/\")[-1]}]\n    return [{\"id\": \"none\", \"name\": \"尚未上傳影像\"}]\n\n@app.get(\"/api/summary\")\nasync def get_summary():\n    if gdf is None or len(gdf) == 0:\n        return {\"vehicle\": 0, \"person\": 0, \"cone\": 0, \"total\": 0}\n    summary = {}\n    for cls in [\"car\", \"person\", \"cone\"]:\n        count = len(gdf[gdf[\"class\"] == cls])\n        summary[CLASS_MAP.get(cls, cls)] = count\n    summary[\"total\"] = len(gdf)\n    return summary\n\n@app.get(\"/api/gpu/status\")\nasync def get_gpu_status():\n    try:\n        if torch.cuda.is_available():\n            return {\"name\": torch.cuda.get_device_name(0), \"status\": \"online\"}\n    except:\n        pass\n    return {\"name\": \"CPU Mode\", \"status\": \"offline\"}\n\n@app.get(\"/api/ortho/bounds\")\nasync def get_ortho_bounds():\n    \"\"\"取得正射影像邊界（WGS84）\"\"\"\n    if src is None:\n        return {\"error\": \"尚未上傳影像\"}\n    \n    try:\n        # 取得影像四個角落的座標\n        transform = src.transform\n        width, height = src.width, src.height\n        \n        # 計算四個角落（像素座標 -> 原始 CRS）\n        corners = [\n            transform * (0, 0),           # 左上\n            transform * (width, 0),       # 右上\n            transform * (0, height),      # 左下\n            transform * (width, height),  # 右下\n        ]\n        \n        xs = [c[0] for c in corners]\n        ys = [c[1] for c in corners]\n        \n        # 轉換到 WGS84\n        transformer = Transformer.from_crs(src.crs, \"EPSG:4326\", always_xy=True)\n        \n        # 轉換四個角落\n        lons, lats = [], []\n        for x, y in corners:\n            lon, lat = transformer.transform(x, y)\n            lons.append(lon)\n            lats.append(lat)\n        \n        return {\n            \"north\": max(lats),\n            \"south\": min(lats),\n            \"east\": max(lons),\n            \"west\": min(lons),\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n\n@app.get(\"/api/ortho/image\")\nasync def get_ortho_image():\n    \"\"\"取得正射影像 PNG\"\"\"\n    global ortho_png_cache\n    \n    if ortho_png_cache is None:\n        if src is not None:\n            generate_ortho_png()\n        \n        if ortho_png_cache is None:\n            return Response(content=b\"\", status_code=404)\n    \n    return Response(\n        content=ortho_png_cache,\n        media_type=\"image/png\",\n        headers={\"Cache-Control\": \"max-age=3600\"}\n    )\n\n@app.get(\"/api/ortho/preview\")\nasync def get_ortho_preview(with_detections: bool = True, width: int = 800, height: int = 600):\n    \"\"\"生成正射影像預覽圖（含偵測結果），用於 PDF 報表\"\"\"\n    global gdf, src\n    \n    if src is None:\n        return Response(content=b\"\", status_code=404)\n    \n    try:\n        # 讀取並縮放影像\n        data = src.read(\n            out_shape=(src.count, height, width),\n            resampling=rasterio.enums.Resampling.bilinear\n        )\n        \n        # 轉換為 RGB\n        if data.shape[0] >= 3:\n            rgb = np.transpose(data[:3], (1, 2, 0))\n        else:\n            rgb = np.stack([data[0]] * 3, axis=-1)\n        \n        # 正規化到 0-255\n        rgb_min, rgb_max = rgb.min(), rgb.max()\n        if rgb_max > rgb_min:\n            rgb = ((rgb - rgb_min) / (rgb_max - rgb_min) * 255).astype(np.uint8)\n        else:\n            rgb = np.zeros((height, width, 3), dtype=np.uint8)\n        \n        # 使用 PIL 繪製\n        img = Image.fromarray(rgb)\n        \n        # 疊加偵測結果\n        if with_detections and gdf is not None and len(gdf) > 0:\n            draw = ImageDraw.Draw(img)\n            \n            # 顏色對應 (RGB)\n            colors = {\n                'car': (59, 130, 246),      # 藍\n                'person': (34, 197, 94),    # 綠\n                'cone': (249, 115, 22),     # 橘\n            }\n            \n            # 計算縮放比例\n            scale_x = width / src.width\n            scale_y = height / src.height\n            \n            for _, row in gdf.iterrows():\n                # 取得 bbox 像素座標並縮放\n                px1, py1, px2, py2 = row[\"bbox_px\"]\n                x1 = int(px1 * scale_x)\n                y1 = int(py1 * scale_y)\n                x2 = int(px2 * scale_x)\n                y2 = int(py2 * scale_y)\n                \n                # 中心點\n                cx = (x1 + x2) // 2\n                cy = (y1 + y2) // 2\n                \n                color = colors.get(row[\"class\"], (100, 100, 100))\n                \n                # 畫框\n                draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n                # 畫中心點\n                r = 4\n                draw.ellipse([cx-r, cy-r, cx+r, cy+r], fill=color, outline=(255, 255, 255))\n        \n        # 輸出 PNG\n        buffer = BytesIO()\n        img.save(buffer, format='PNG')\n        buffer.seek(0)\n        \n        return Response(\n            content=buffer.getvalue(),\n            media_type=\"image/png\",\n            headers={\n                \"Cache-Control\": \"no-cache\",\n                \"Access-Control-Allow-Origin\": \"*\"\n            }\n        )\n        \n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return Response(content=str(e).encode(), status_code=500)\n\n@app.get(\"/api/ortho/metadata\")\nasync def get_ortho_metadata():\n    \"\"\"取得 TIFF 元資料\"\"\"\n    if src is None:\n        return {\"error\": \"尚未上傳影像\"}\n    \n    try:\n        # 嘗試從 TIFF 取得時間資訊\n        datetime_str = None\n        if hasattr(src, 'tags'):\n            tags = src.tags()\n            datetime_str = tags.get('TIFFTAG_DATETIME') or tags.get('datetime')\n        \n        return {\n            \"filename\": ORTHO_TIF.split(\"/\")[-1] if ORTHO_TIF else \"unknown\",\n            \"datetime\": datetime_str,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"crs\": str(src.crs) if src.crs else None,\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n\n# ----- 啟動伺服器 -----\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n\ndef run_tunnel():\n    process = subprocess.Popen(\n        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8000\"],\n        stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n    )\n    for line in process.stderr:\n        match = re.search(r\"https://[a-zA-Z0-9-]+\\.trycloudflare\\.com\", line)\n        if match:\n            print(f\"\\n{'='*60}\")\n            print(f\"🚀 API 伺服器已啟動！\")\n            print(f\"📡 公開網址: {match.group()}\")\n            print(f\"{'='*60}\")\n            print(f\"\\n📋 使用流程:\")\n            print(f\"   1. 複製上面網址到前端 Connect\")\n            print(f\"   2. 前端上傳影像\")\n            print(f\"   3. 按 RUN 開始偵測\")\n            break\n\nprint(\"正在啟動伺服器...\")\nthreading.Thread(target=run_server, daemon=True).start()\ntime.sleep(2)\nprint(\"正在建立 Cloudflare Tunnel...\")\nthreading.Thread(target=run_tunnel, daemon=True).start()\n\nwhile True:\n    time.sleep(1)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "ds5h9LtyoKz6",
    "outputId": "d345a5cb-b37d-40bf-e443-505ea73d3193"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}